1.
Question 1
At InnovateAI, a team of AI engineers led by Sarah is tasked with developing an image classification model using Keras. They decide to leverage vision transformers and transfer learning. What should be their initial step in implementing this model?



Implement a convolutional neural network as a baseline before using vision transformers



Design a custom vision transformer architecture from scratch



Collect a large dataset to train the vision transformer from scratch



Load a pre-trained vision transformer model and prepare the dataset for transfer learning


1 point
2.
Question 2
Imagine that at AI Solutions Corp., the team led by Jamie is tasked with summarizing the use of vision transformers in Keras. What aspect should they emphasize to demonstrate the application of transfer learning in computer vision tasks?



The necessity of large datasets to train vision transformers effectively



The exclusive use of convolutional layers in vision transformers



The ability to adapt pre-trained models for specific tasks with minimal data



The requirement for extensive hyperparameter tuning in every application


1 point
3.
Question 3
At TechVision Inc., Alex is implementing a vision transformer model using PyTorch for a domain-specific image dataset. Which key step should Alex focus on to effectively apply transfer learning?



Use the model without any modifications on the new dataset



Fine-tune the pre-trained model on the new dataset



Implement additional layers to the existing model



Train the model from scratch with the new dataset


1 point
4.
Question 4
What is the function of the self.features module inside the ConvNet class?



It loads the dataset into the model.



It computes the loss for the model during training.



It implements the transformer encoder for the hybrid architecture.



It applies the pre-trained model architecture for CNN-based feature extraction.


1 point
5.
Question 5
What is the primary operation performed by the PatchEmbed class's proj layer in the Vision Transformer implementation?



Projects the input feature map to a lower-dimensional embedding using a 1×1 convolution



Applies a pooling operation to flatten patches



Flattens the original image directly into a sequence



Divides the image into overlapping patches using a 3×3 convolution


1 point
6.
Question 6
When creating the positional encoding for image patches, which of the following operations is typically performed?



Subtracting the patch mean from each patch



Concatenating zeros to patch vectors



Adding a learned or fixed vector to each patch embedding 



Reshuffling the patches randomly each epoch


1 point
7.
Question 7
During Vision Transformer training, what is the typical purpose of including a "Classification Head" at the end of the model?



To resize the image to its original shape



To encode positional information



To project the final encoder output to the number of target classes



To increase model regularization


1 point
